{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_output\n",
    "\n",
    "#%matplotlib inline\n",
    "datax = pd.read_csv('../Toru/train_quora_toy2.csv').fillna(\"\")\n",
    "#datax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.corpus import wordnet\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "STOP_WORDS = nltk.corpus.stopwords.words()\n",
    "\n",
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    #sentence = regex.sub('', val).lower()\n",
    "    sentence1 = regex.sub('', val)\n",
    "    \n",
    "    sentence1 = sentence1.split(\" \")\n",
    "    \n",
    "    for word in list(sentence1):\n",
    "        if word in STOP_WORDS:\n",
    "            sentence1.remove(word)  \n",
    "            \n",
    "    sentence1 = \" \".join(sentence1)\n",
    "    \n",
    "    # doing some substitutions after stopword removal and lower case\n",
    "    sentence1 = re.sub(r'india', 'India', sentence1)\n",
    "    sentence1 = re.sub(r'banglore', 'Bangalore', sentence1)\n",
    "    sentence1 = re.sub(r'bangalore', 'Bangalore', sentence1)\n",
    "    sentence1 = re.sub(r'pune', 'Pune', sentence1)\n",
    "    sentence1 = re.sub(r'usa',  'United States', sentence1)\n",
    "    sentence1 = re.sub(r'united states',  'United States', sentence1)\n",
    "    sentence1 = re.sub(r'united states of america',  'United States', sentence1)\n",
    "    #text = re.sub(r\" USA \", \" America \", text)\n",
    "    #text = re.sub(r\" u s \", \" America \", text)\n",
    "    #text = re.sub(r\" uk \", \" England \", text)\n",
    "    #text = re.sub(r\" UK \", \" England \", text)\n",
    "    #text = re.sub(r\"india\", \"India\", text)\n",
    "    #text = re.sub(r\"switzerland\", \"Switzerland\", text)\n",
    "    #text = re.sub(r\"china\", \"China\", text)\n",
    "    \n",
    "    return sentence1\n",
    "\n",
    "def clean_dataframe(datax):\n",
    "    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n",
    "    datax = datax.dropna(how=\"any\")\n",
    "    \n",
    "    for col in ['question1', 'question2']:\n",
    "        datax[col] = datax[col].apply(clean_sentence)\n",
    "    \n",
    "    return datax\n",
    "\n",
    "datax = clean_dataframe(datax)\n",
    "#print(datax)\n",
    "##### Dropping a column created in later  cell ####\n",
    "#data.drop('ques1', axis=1, inplace=True)\n",
    "#data = data.drop('ques1', 1)\n",
    "\n",
    "#places = GeoText('string')\n",
    "#print(places.countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def build_corpus(datax):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = []\n",
    "    for col in ['question1', 'question2']:\n",
    "        for sentence1 in datax[col].iteritems():\n",
    "            word_list = sentence1[1].split(\" \")\n",
    "            corpus.append(word_list)\n",
    "            \n",
    "    return corpus\n",
    "\n",
    "corpus = build_corpus(datax)        \n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "[0.17, 0.17, 0.17, 0.17, 0, 0.17, 0, 1, 0.17, 0.17, 0.17, 1, 1, 1, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17]\n"
     ]
    }
   ],
   "source": [
    "# changing logger level so that \" under 10 batch_words\" error does not pop up\n",
    "import logging\n",
    "logging.getLogger('gensim.models.word2vec').setLevel(logging.ERROR)\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "# ignoring an warning\n",
    "import warnings\n",
    "#warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "#warnings.filterwarnings('ignore', '.*gensim.models.word2vec:under 10.*',)\n",
    "import gensim\n",
    "\n",
    "#print(len(corpus))\n",
    "\n",
    "\n",
    "out_df = pd.DataFrame()\n",
    "is_duplicate =[]\n",
    "# Finding question pairs\n",
    "for i in range(0, int((len(corpus))/2)):\n",
    "    if i%5==0: \n",
    "        print(i)\n",
    "    pair1 = list( corpus[i] for i in [i, i+ int((len(corpus))/2)] )\n",
    "    pair1[0][0]=pair1[0][0].lower()\n",
    "    pair1[1][0]=pair1[1][0].lower()\n",
    "    #print(pair1)\n",
    "    model = gensim.models.Word2Vec(pair1, min_count=1, workers=4)\n",
    "    vec_avg1 = sum(model.wv[pair1[0]])/(len(pair1[0]))\n",
    "    vec_avg2 = sum(model.wv[pair1[1]])/(len(pair1[1]))\n",
    "    def cosine_similarity(vec_avg1,vec_avg2):\n",
    "        \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "        sumxx, sumxy, sumyy = 0, 0, 0\n",
    "        for j in range(len(vec_avg1)):\n",
    "            x = vec_avg1[j]; y = vec_avg2[j]\n",
    "            sumxx += x*x\n",
    "            sumyy += y*y\n",
    "            sumxy += x*y\n",
    "        return sumxy/math.sqrt(sumxx*sumyy)\n",
    "    #print(cosine_similarity(vec_avg1,vec_avg2))\n",
    "    sim = cosine_similarity(vec_avg1,vec_avg2)\n",
    "    if sim < 0: sim =0\n",
    "        \n",
    "    if pair1[0] ==[''] or pair1[1]==['']: # if any one question of the pair is empty then sim=0\n",
    "        sim=0\n",
    "    else:\n",
    "        try:\n",
    "            pos1 = nltk.pos_tag(pair1[0])\n",
    "            #print(pos1)\n",
    "        except IndexError:\n",
    "            sim=sim\n",
    "        try:\n",
    "            pos2 = nltk.pos_tag(pair1[1])\n",
    "            #print(pos2)\n",
    "        except IndexError:\n",
    "            sim=sim\n",
    "        a = list()\n",
    "        for item in pos1:\n",
    "            if item[1] == 'NNP' or item[1]=='NN' or item[1]=='NNS' or item[1]=='NNPS': # considering any POS tag that starts with 'NN'\n",
    "              a.append(item[0])\n",
    "        if a != []:\n",
    "            a = [item.lower() for item in a]\n",
    "        else:\n",
    "            sim=sim\n",
    "        #print(a)\n",
    "        b = list()\n",
    "        for item in pos2:\n",
    "            if item[1] == 'NNP' or item[1]=='NN' or item[1]=='NNS' or item[1]=='NNPS':\n",
    "              b.append(item[0])\n",
    "        if b != []:\n",
    "            b = [item.lower() for item in b]\n",
    "        else:\n",
    "            sim=sim\n",
    "        #print(b)\n",
    "\n",
    "        if a==[] and b==[]:\n",
    "            sim=0\n",
    "        else:\n",
    "            compare = lambda a, b: collections.Counter(a) == collections.Counter(b)\n",
    "            #print(compare(a,b))\n",
    "            if compare(a,b) == True:\n",
    "                sim=1\n",
    "            else:\n",
    "                sim=0.17\n",
    "    \n",
    "    #print(sim)\n",
    "    #out_df = pd.DataFrame({'is_duplicate': [sim]})\n",
    "    is_duplicate.append(sim)\n",
    "    i = i+1\n",
    "\n",
    "#out_df.index.names = ['test_id'] \n",
    "out_df['test_id'] = datax['id']\n",
    "out_df['is_duplicate'] = is_duplicate\n",
    "out_df.to_csv('toy3_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('laws', 'NNS'), ('change', 'VBP'), ('status', 'NN'), ('student', 'NN'), ('visa', 'NN'), ('green', 'JJ'), ('card', 'NN'), ('us', 'PRP'), ('compare', 'VBP'), ('immigration', 'JJ'), ('laws', 'NNS'), ('Canada', 'NNP')]\n",
      "[('laws', 'NNS'), ('change', 'VBP'), ('status', 'NN'), ('student', 'NN'), ('visa', 'NN'), ('green', 'JJ'), ('card', 'NN'), ('us', 'PRP'), ('compare', 'VBP'), ('immigration', 'JJ'), ('laws', 'NNS'), ('Japan', 'NNP')]\n",
      "Counter({'NN': 4, 'NNS': 2, 'VBP': 2, 'JJ': 2, 'NNP': 1, 'PRP': 1})\n",
      "Counter({'NN': 4, 'NNS': 2, 'VBP': 2, 'JJ': 2, 'NNP': 1, 'PRP': 1})\n",
      "Counter({'NNP': 0, 'NN': 0, 'NNS': 0, 'PRP': 0, 'VBP': 0, 'JJ': 0})\n",
      "['Canada']\n",
      "['Japan']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cell is used to find proper Noun-based similarity for the above cell\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "s1 = [['step', 'step', 'guide', 'invest', 'share', 'market', 'india'], ['step', 'step', 'guide', 'invest', 'share', 'market']]\n",
    "s3 = [['story', 'kohinoor', 'kohinoor', 'diamond'], \n",
    "['would','happen','indian','government','stole','kohinoor','kohinoor','diamond','back']] \n",
    "s5 = [['make', 'physics', 'easy', 'learn'], ['make', 'physics', 'easy', 'learn']]\n",
    "#s6 = ['make', 'physics', 'easy', 'learn']\n",
    "s7 = [['first', 'sexual', 'experience', 'like'], ['first', 'sexual', 'experience']]\n",
    "s9 = [['good', 'geologist'], ['great', 'geologist']]\n",
    "s11= [['laws', 'change', 'status', 'student', 'visa', 'green', 'card', 'us', 'compare', 'immigration', 'laws', 'Canada'], \n",
    "      ['laws', 'change', 'status', 'student', 'visa', 'green', 'card', 'us', 'compare', 'immigration', 'laws', 'Japan']]\n",
    "s13 = [['What', 'make', 'Physics', 'easy', 'learn'], ['How', 'make', 'physics', 'easy', 'learn']]\n",
    "\n",
    "#model = Word2Vec(pairs1, size=100, window=20, min_count=200, workers=4)\n",
    "model = gensim.models.Word2Vec(list(s9), min_count=1)\n",
    "\n",
    "#print(model)\n",
    "#model.wv['like']\n",
    "#print(model.wv[s9[0]])\n",
    "\n",
    "# parts-of-speech tag\n",
    "pos1 = nltk.pos_tag(s11[0])\n",
    "print(pos1)\n",
    "pos2 = nltk.pos_tag(s11[1])\n",
    "print(pos2)\n",
    "\n",
    "count1= Counter([j for i,j in pos1])\n",
    "print(count1)\n",
    "count2= Counter([j for i,j in pos2])\n",
    "print(count2)\n",
    "\n",
    "#pos_difx = count1 - count2\n",
    "#print(pos_difx)\n",
    "\n",
    "count1.subtract(count2)\n",
    "print(count1)\n",
    "\n",
    "a = list()\n",
    "for item in pos1:\n",
    "    if item[1] == 'NNP': # considering any POS tag that starts with 'N'\n",
    "      a.append(item[0])\n",
    "print(a)\n",
    "b = list()\n",
    "for item in pos2:\n",
    "    if item[1] == 'NNP':\n",
    "      b.append(item[0])\n",
    "print(b)\n",
    "\n",
    "compare = lambda a, b: collections.Counter(a) == collections.Counter(b)\n",
    "compare(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
